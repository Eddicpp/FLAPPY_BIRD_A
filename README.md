# FLAPPY_BIRD_A

> **Autonomous AI agent learning to play Flappy Bird using Reinforcement Learning on embedded hardware**

![Arduino](https://img.shields.io/badge/Arduino-Mega%202560-00979D?logo=arduino&logoColor=white)
![Algorithm](https://img.shields.io/badge/Algorithm-Q--Learning-orange)
![Language](https://img.shields.io/badge/Language-C%2B%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Complete-success)

A complete implementation of **tabular Q-Learning** from scratch on Arduino Mega, demonstrating reinforcement learning principles on resource-constrained embedded hardware. The AI learns through trial and error, discovering optimal strategies to navigate obstacles and maximize survival time.

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Demo](#-demo)
- [How It Works](#-how-it-works)
- [Hardware Requirements](#-hardware-requirements)
- [Installation](#-installation)
- [Training Progress](#-training-progress)
- [Technical Details](#-technical-details)
- [Configuration](#-configuration)
- [Troubleshooting](#-troubleshooting)
- [Future Work](#-future-work)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## ğŸ¯ Overview

This project implements a **Q-Learning agent** that autonomously learns to play Flappy Bird on an Arduino Mega 2560. The AI uses no pre-programmed rules or human demonstrationsâ€”it learns purely through interaction with the environment, receiving rewards for successful actions and penalties for failures.

**Key Achievement:** The AI converges to expert-level play within 500-1000 episodes, demonstrating effective reinforcement learning on an 8-bit microcontroller with only 8KB of RAM.

### Why This Project Matters

- ğŸ§  **Educational**: Clear implementation of RL fundamentals
- ğŸ’¾ **Embedded AI**: Demonstrates ML on resource-constrained devices
- ğŸ® **Interactive**: Visual real-time learning progression
- ğŸ“Š **Reproducible**: Complete code with documented hyperparameters
- ğŸ”¬ **Extensible**: Foundation for more complex embedded AI projects

---

## âœ¨ Features

### Reinforcement Learning
- âœ… **Tabular Q-Learning** with Bellman equation updates
- âœ… **Epsilon-greedy exploration** (40% â†’ 1% decay)
- âœ… **512-state discretization** for efficient state representation
- âœ… **3-action space**: Jump, Stay, Speed Boost
- âœ… **Dense reward shaping** for faster convergence
- âœ… **EEPROM persistence** - training survives power cycles

### Memory Optimization
- ğŸ¯ **Integer Q-table** (int16) vs float â†’ 50% RAM reduction
- ğŸ¯ **Scaled arithmetic** for hyperparameters (Ã—100)
- ğŸ¯ **Efficient state encoding** (single index calculation)
- ğŸ¯ **Total footprint**: 3KB Q-table + 2KB game = 5KB/8KB used

### Game Mechanics
- ğŸ® Real-time physics at 60 FPS
- ğŸ® Procedural pipe generation
- ğŸ® **Speed boost** (4Ã— speed, 100ms duration)
- ğŸ® Strategic boost rewards/penalties
- ğŸ® Collision detection & boundary checks
- ğŸ® Live statistics display (score, episode, epsilon)

---

## ğŸ¥ Demo

### Training Progression

**Episode 1**: Random flapping, immediate crashes
```
Score: 0 | AI flaps randomly
ğŸ¦ â†’ ğŸ’¥ (crashes into first pipe)
```

**Episode 100**: Learning obstacle avoidance
```
Score: 8 | AI starts following gaps
ğŸ¦ â†’ ğŸ“Š â†’ ğŸ“Š â†’ ğŸ’¥
```

**Episode 500**: Expert play with boost strategy
```
Score: 35 | AI uses speed strategically
ğŸ¦ â†’â†’ ğŸ“Š â†’â†’â†’â†’ ğŸ“Š â†’â†’ ğŸ“Š âœ“
```

### Video Demo


---

## ğŸ§  How It Works

### 1. State Space Discretization

The continuous game world is discretized into **512 discrete states**:
```
State = f(BirdY, DistanceToPipe, GapPosition)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bird Y Position: 8 buckets (8 pixels each)     â”‚
â”‚  Distance to Pipe: 8 buckets (16 pixels each)   â”‚
â”‚  Gap Position: 8 buckets (top/mid/bottom)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total States: 8 Ã— 8 Ã— 8 = 512
```

**State Index Formula:**
```cpp
stateIndex = (yBucket * 64) + (distBucket * 8) + gapBucket
```

### 2. Q-Learning Algorithm

The AI learns action values using the **Bellman equation**:
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
                           a'
```

**Components:**
- `Q(s,a)`: Expected future reward for action `a` in state `s`
- `Î± = 0.15`: Learning rate (how much to update)
- `Î³ = 0.95`: Discount factor (future reward importance)
- `r`: Immediate reward received
- `max Q(s',a')`: Best expected reward from next state

**Implementation (Integer Math):**
```cpp
int maxNextQ = max(max(Q[newState][0], Q[newState][1]), Q[newState][2]);
long delta = reward + ((long)discountFactor * maxNextQ / 100) - Q[oldState][action];
Q[oldState][action] += (int)((long)learningRate * delta / 100);
```

### 3. Action Selection: Epsilon-Greedy

Balances **exploration** (trying new actions) vs **exploitation** (using learned knowledge):
```cpp
if (random(100) < epsilon) {
    // EXPLORE: Random action
    action = random(3);
} else {
    // EXPLOIT: Best known action
    action = argmax(Q[state][0], Q[state][1], Q[state][2]);
}
```

**Epsilon decay:** `Îµ = Îµ Ã— 0.996` per episode (40% â†’ 1%)

### 4. Reward Function

**Multi-component dense reward** guides learning:

| Event | Reward | Rationale |
|-------|--------|-----------|
| **Pass pipe** | +100 | Primary objective |
| **Death** | -100 | Strong discouragement |
| **Alive (aligned)** | +1 to +8 | Shaped by positioning |
| **Alignment bonus** | +4 | Encourages centering on gap |
| **Proximity bonus** | +3 | Rewards safe approach |
| **Edge penalty** | -5 | Discourages risky flight |
| **Velocity penalty** | -1 | Penalizes erratic movement |
| **Boost near pipe** | -5 | Dangerous high speed |
| **Boost far from pipe** | +2 | Smart acceleration |

### 5. Speed Boost Mechanic

AI learns **when and where** to use speed boost:
- **Duration**: 100ms
- **Speed multiplier**: 4Ã— (2 px/frame â†’ 8 px/frame)
- **Pipe spawn rate**: 4Ã— faster (proportional)
- **Reward structure**: Penalized near pipes, rewarded in open space

---

## ğŸ”§ Hardware Requirements

### Components

| Item | Specification | Quantity | Notes |
|------|--------------|----------|-------|
| **Microcontroller** | Arduino Mega 2560 | 1 | 8KB SRAM required |
| **Display** | OLED 128Ã—64 I2C SSD1306 | 1 | I2C address 0x3C |
| **Cables** | Dupont jumper wires | 4 | For I2C connection |

### Wiring Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Arduino Mega    â”‚         â”‚  OLED Display    â”‚
â”‚                  â”‚         â”‚  128Ã—64 I2C      â”‚
â”‚                  â”‚         â”‚                  â”‚
â”‚  5V    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  VCC             â”‚
â”‚  GND   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  GND             â”‚
â”‚  Pin 20 (SDA) â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  SDA             â”‚
â”‚  Pin 21 (SCL) â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  SCL             â”‚
â”‚                  â”‚         â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Required Libraries
```cpp
#include <Wire.h>              // Built-in I2C library
#include <Adafruit_GFX.h>      // Graphics core
#include <Adafruit_SSD1306.h>  // OLED driver
#include <EEPROM.h>            // Built-in persistent storage
```

---

## ğŸ“¥ Installation

### Step 1: Install Arduino IDE

Download and install from [arduino.cc](https://www.arduino.cc/en/software)

### Step 2: Install Required Libraries

**Method A: Library Manager (Recommended)**
1. Open Arduino IDE
2. Go to `Sketch` â†’ `Include Library` â†’ `Manage Libraries...`
3. Search and install:
   - **Adafruit GFX Library** by Adafruit
   - **Adafruit SSD1306** by Adafruit

**Method B: Manual Installation**
```bash
cd ~/Documents/Arduino/libraries/
git clone https://github.com/adafruit/Adafruit-GFX-Library.git
git clone https://github.com/adafruit/Adafruit_SSD1306.git
```

### Step 3: Clone This Repository
```bash
git clone https://github.com/yourusername/flappy-bird-qlearning.git
cd flappy-bird-qlearning
```

### Step 4: Upload to Arduino

1. Connect Arduino Mega via USB
2. Open Arduino IDE
3. Select: `Tools` â†’ `Board` â†’ `Arduino Mega or Mega 2560`
4. Select: `Tools` â†’ `Processor` â†’ `ATmega2560`
5. Select correct COM port: `Tools` â†’ `Port` â†’ `COM X`
6. Open `FlappyBird_QLearning.ino`
7. Click **Upload** button (â†’)

### Step 5: Monitor Training

1. Open Serial Monitor: `Tools` â†’ `Serial Monitor`
2. Set baud rate to **9600**
3. Watch the AI learn in real-time!

---

## ğŸ“ˆ Training Progress

### Serial Monitor Output
```
=== FLAPPY BIRD Q-LEARNING ===
Loading Q-table from EEPROM...
Loaded! Ep: 0 | Best: 0 | Îµ: 40

Episode 1    | Score: 0  | High: 0  | Îµ: 40%
Episode 10   | Score: 1  | High: 2  | Îµ: 36%
Episode 50   | Score: 3  | High: 5  | Îµ: 28%
Episode 100  | Score: 8  | High: 11 | Îµ: 21%
Saving Q-table...
Saved!
Episode 200  | Score: 15 | High: 18 | Îµ: 13%
>>> NEW HIGH SCORE!
Episode 500  | Score: 32 | High: 38 | Îµ: 4%
Episode 1000 | Score: 41 | High: 47 | Îµ: 1%
```

### Learning Curve
```
Score
  50 â”‚                                    â•­â”€â”€
  45 â”‚                                â•­â”€â”€â”€â•¯
  40 â”‚                            â•­â”€â”€â”€â•¯
  35 â”‚                        â•­â”€â”€â”€â•¯
  30 â”‚                   â•­â”€â”€â”€â”€â•¯
  25 â”‚              â•­â”€â”€â”€â”€â•¯
  20 â”‚         â•­â”€â”€â”€â”€â•¯
  15 â”‚    â•­â”€â”€â”€â”€â•¯
  10 â”‚â•­â”€â”€â”€â•¯
   5 â”‚â•¯
   0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Episodes
     0   100  200  300  400  500  600  700  800
```

### Performance Milestones

| Episodes | Avg Score | Epsilon | Behavior Description |
|----------|-----------|---------|---------------------|
| 1-50 | 0-2 | 40-32% | Random flapping, learns nothing |
| 50-100 | 2-5 | 32-26% | Discovers jumping avoids death |
| 100-200 | 5-12 | 26-17% | Learns to navigate gaps |
| 200-300 | 12-20 | 17-11% | Improves timing precision |
| 300-500 | 20-32 | 11-4% | Develops boost strategy |
| 500+ | 32-45+ | <4% | Expert play, consistent survival |

---

## ğŸ”¬ Technical Details

### Memory Footprint
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Arduino Mega 2560 SRAM: 8,192 bytes       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Q-table (512Ã—3Ã—2):        3,072 bytes     â”‚  37.5%
â”‚  Display buffer (1024):    1,024 bytes     â”‚  12.5%
â”‚  Game variables:             300 bytes     â”‚   3.7%
â”‚  Physics & stack:            500 bytes     â”‚   6.1%
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  USED:                     4,896 bytes     â”‚  59.8%
â”‚  FREE:                     3,296 bytes     â”‚  40.2% âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### EEPROM Memory Map

**Persistent storage layout (4KB EEPROM):**

| Address | Size | Content | Purpose |
|---------|------|---------|---------|
| 0-3 | 4 bytes | Magic: `0xABCD` | Validation marker |
| 4-7 | 4 bytes | Episode count | Training progress |
| 8-11 | 4 bytes | Best score | High score record |
| 12-15 | 4 bytes | Epsilon value | Exploration rate |
| 16-3087 | 3072 bytes | Q-table | Learned values |

**Auto-save:** Q-table saved every episode

### Hyperparameters
```cpp
// Learning dynamics
Learning Rate (Î±):      0.15   // How fast to learn (15/100)
Discount Factor (Î³):    0.95   // Future reward importance (95/100)

// Exploration schedule
Epsilon Initial:        0.40   // 40% random actions
Epsilon Decay:          0.996  // Multiply per episode
Epsilon Minimum:        0.01   // 1% floor (always explore)

// State space
States:                 512    // 8 Ã— 8 Ã— 8 discretization
Actions:                3      // Jump, Stay, Boost

// Physics
Gravity:                0.35   // Downward acceleration
Jump Power:            -2.0    // Upward velocity on jump
Pipe Speed:             2 px/frame (normal)
Pipe Speed Boost:       8 px/frame (4Ã— multiplier)
Pipe Gap:               24 pixels
Frame Rate:             ~60 FPS
```

### State Encoding Details

**Bird Y Position (8 buckets):**
```
Bucket 0: Y = 0-7    (top)
Bucket 1: Y = 8-15
Bucket 2: Y = 16-23
...
Bucket 7: Y = 56-63  (bottom)
```

**Distance to Pipe (8 buckets):**
```
Bucket 0: Dist = 0-15    (very close)
Bucket 1: Dist = 16-31
...
Bucket 7: Dist = 112-127 (far)
```

**Gap Position (8 buckets):**
```
Bucket 0-2: Top third    (gap high)
Bucket 3-5: Middle third (gap center)
Bucket 6-7: Bottom third (gap low)
```

---

## âš™ï¸ Configuration

### Adjusting Difficulty

**Make it easier:**
```cpp
#define PIPE_GAP 32           // Wider gap (was 24)
#define PIPE_INTERVAL 3000    // More time between pipes
#define GRAVITY 0.25          // Slower falling
```

**Make it harder:**
```cpp
#define PIPE_GAP 20           // Narrower gap
#define PIPE_INTERVAL 2000    // Less time between pipes
#define GRAVITY 0.45          // Faster falling
```

### Training Speed

**Faster training (higher exploration):**
```cpp
int epsilon = 60;             // Start at 60%
int epsilonDecay = 995;       // Slower decay
```

**Slower training (more conservative):**
```cpp
int epsilon = 20;             // Start at 20%
int epsilonDecay = 998;       // Faster decay
```

### Reset Training

To start fresh:
```cpp
void setup() {
  // Uncomment these lines to reset EEPROM
  for (int i = 0; i < EEPROM.length(); i++) {
    EEPROM.write(i, 0);
  }
}
```

---

## ğŸ› Troubleshooting

### Display Not Working

**Issue:** Blank screen or no display
```
Solutions:
1. Check I2C address (try 0x3C or 0x3D)
2. Verify wiring (SDAâ†’20, SCLâ†’21)
3. Test I2C scanner sketch
4. Ensure 5V power to display
```

### AI Not Learning

**Issue:** Score stays at 0 after 100+ episodes
```
Solutions:
1. Verify epsilon is decaying (check Serial)
2. Increase learning rate: learningRate = 20
3. Simplify reward (remove penalties)
4. Reset Q-table from EEPROM
5. Check state discretization (print states)
```

### Compilation Errors

**Issue:** `error: 'display' was not declared`
```
Solutions:
1. Install Adafruit_GFX library
2. Install Adafruit_SSD1306 library
3. Restart Arduino IDE
```

**Issue:** `Low memory available`
```
Solutions:
1. Using Arduino Mega? (Uno has insufficient RAM)
2. Reduce TOTAL_STATES if needed
3. Comment out Serial.print() statements
```

### Serial Monitor Issues

**Issue:** Garbled output
```
Solutions:
1. Set baud rate to 9600
2. Check COM port selection
3. Close other Serial programs
```

---

## ğŸš€ Future Work

### Algorithmic Improvements
- [ ] **Double Q-Learning** to reduce value overestimation
- [ ] **SARSA** implementation for on-policy comparison
- [ ] **Eligibility traces** (Q(Î»)) for faster credit assignment
- [ ] **Prioritized sweeping** to update important states first

### Feature Engineering
- [ ] Add enemy paddle position awareness (inspired by Pong project)
- [ ] Include velocity in state representation
- [ ] Multi-objective rewards (time bonus, style points)
- [ ] Curriculum learning (gradually increase difficulty)

### Embedded ML Extensions
- [ ] **Function approximation** (tile coding on Arduino Due)
- [ ] **Neural network Q-learning** (simple 2-layer MLP)
- [ ] **Transfer learning** to new game variants
- [ ] **Multi-task learning** across different games

### Data & Visualization
- [ ] Export training data via SD card module
- [ ] Python visualization script for learning curves
- [ ] Q-table heatmap generator
- [ ] Web dashboard for real-time monitoring

### Hardware Scaling
- [ ] **Distributed learning** across multiple Arduinos
- [ ] **Pong AI vs AI** (2-agent competitive learning)
- [ ] Cloud connectivity for data aggregation
- [ ] FPGA acceleration for larger networks

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

### Areas for Contribution
- ğŸ› **Bug fixes** and optimization
- ğŸ“š **Documentation** improvements
- ğŸ§ª **Experiments** with hyperparameters
- ğŸ“Š **Visualization** tools
- ğŸ® **New game mechanics**
- ğŸ”¬ **Algorithm** implementations

### Contribution Process
1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Code Style
- Follow existing Arduino naming conventions
- Comment complex logic
- Test on actual hardware before PR
- Update README if adding features

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.
```
MIT License - Copyright (c) 2024

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software to use, modify, and distribute for any purpose.
```

---

## ğŸ™ Acknowledgments

### Inspiration & Theory
- **Sutton & Barto** - *Reinforcement Learning: An Introduction* (2018)
- **Watkins** - Original Q-Learning paper (1989)
- **DeepMind** - DQN algorithm inspiration

### Libraries & Tools
- **Adafruit** - Excellent graphics and OLED libraries
- **Arduino** - Open-source embedded platform
- **Flappy Bird** - Original game by Dong Nguyen

### Community
- Arduino forums for embedded optimization tips
- RL community for algorithm insights
- GitHub for hosting and collaboration

---

## ğŸ“¬ Contact

**Eduardo Pane**
- ğŸ“§ Email: eduardo.pane04@example.com

**Project Link:** [https://github.com/yourusername/flappy-bird-qlearning](https://github.com/yourusername/flappy-bird-qlearning)

---

## â­ Show Your Support

If you found this project interesting or helpful:
- Give it a â­ on GitHub
- Share it with others learning RL
- Fork it and build something cool!
- Cite it in your own projects

---

<div align="center">

### ğŸ¦ Made with passion for AI and embedded systems

**Learning one episode at a time**

[Report Bug](https://github.com/yourusername/flappy-bird-qlearning/issues) Â· 
[Request Feature](https://github.com/yourusername/flappy-bird-qlearning/issues) Â· 
[Discuss](https://github.com/yourusername/flappy-bird-qlearning/discussions)

</div>
